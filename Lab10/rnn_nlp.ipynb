{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWy4zH4pZhDJ"
   },
   "source": [
    "## Language model with RNNs\n",
    "This code is based on Andrej Karpathy's [char-rnn](https://github.com/karpathy/char-rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2yZoH4NxZhDP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ald3-MQZhDS"
   },
   "source": [
    "The Shakespearean text is provided in ./tinyshakespeare.txt. We will load the data and perform some preprocessing for starters. If you are on Colab, you need to re-upload the files when you kill the Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m4B86OE6ZhDS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the contents of the text file\n",
    "data = open('./tinyshakespeare.txt', 'r').read()\n",
    "print (data[:278]) # let's examine some text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9CPbA0iMwCC"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1Njn-BrZhDT"
   },
   "source": [
    "The text looks good but it can not be processed by RNNs in its raw form. We first need to tokenize the data and convert it into a form suitable for RNNs. Since we focus on character level language models in this exercise, we will consider each character as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5dDWBhURhAs"
   },
   "outputs": [],
   "source": [
    "# Bonus: when you have finished, change for word-based processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y4NNxD9c7fRt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text file has 1115394 characters out of which 65 are unique.\n",
      "['w', '&', 'X', 'f', 'H', 'm', '-', 'd', 'L', ',', '$', 'n', '!', \"'\", '?', 'b', 'D', 'N', 'P', 'y', 'M', 'T', 'J', 'l', 'G', ';', '.', 'B', 'U', ' ', 'g', 'k', 'Q', 'i', 'F', 'V', 'K', 'z', 'u', 'r', 'a', 'v', 'E', '3', 't', 'x', 'W', 'e', 'p', 'j', 'C', 'c', 'A', 'Y', ':', 'R', 'O', 'o', 'S', 'Z', '\\n', 'I', 'q', 's', 'h']\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('The text file has {} characters out of which {} are unique.'.format(data_size, vocab_size))\n",
    "print (chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aff5iobZhDU"
   },
   "source": [
    "We have quite a big text with 65 unique characters. Now, we need to associate each character with a unique id which can then be converted into 1-hot vector form to provide as input to the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbADl_VeZhDU"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each character to a unique id and vice versa\n",
    "char_to_ix = ...\n",
    "ix_to_char = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rI6GHdIZhDV"
   },
   "source": [
    "Now that we have a unique id for each character, we can represent each character with a 1-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtUHiWQud99d"
   },
   "outputs": [],
   "source": [
    "def int_to_onehot(..., ...):\n",
    "  return ... # hint: use torch.eye for faster computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHJ_9kL-ZhDX"
   },
   "source": [
    "In the last exercise on classification with MNIST and CIFAR, we had ground truth labels provided explicitly with each instance of the dataset. In our text dataset, we don't have explicitly ground truth labels but note that in a character level language model, we predict the next character. So, in essence, our text is itself the ground truth label since for each character, the next character acts as the ground truth. This will be important when loading the data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf7S9N7kZhDY"
   },
   "source": [
    "PyTorch's DataLoader takes in a specific object of the inbuilt [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class. So, we first need to convert our dataset in this form by inheriting from the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaTI4jDzZhDZ"
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TO-DO: Implement the __getitem__ of the Shakespeare class. \n",
    "# Important points: __getitem__ is called at each training iteration\n",
    "# So, we need to return the data and ground-truth label. The data is in\n",
    "# the form of one-hot vectors and ground-truth is the index of next char\n",
    "# Our RNN operates on an input sequence of a specified length (seq_length)\n",
    "# so we need to return a sequence of one-hot vector and the indices of\n",
    "# their corresponding next character\n",
    "#########################################################################\n",
    "\n",
    "class Shakespeare(Dataset):\n",
    "    \n",
    "    def __init__(self, text_data, seq_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.data = text_data\n",
    "        self.data_size = data_size        \n",
    "        self.chars = chars\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.char_to_ix = char_to_ix\n",
    "        self.ix_to_char = ix_to_char\n",
    "        \n",
    "        # hint: preprocess data for faster loading\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size - self.seq_length - 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X93mffDmZhDa"
   },
   "source": [
    "Now that we have defined our dataset, we can instatiate it and build our dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNLwv_NYZhDa"
   },
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "batch_size = 100\n",
    "\n",
    "dataset = Shakespeare(data, seq_length)\n",
    "vocab_size = dataset.vocab_size\n",
    "\n",
    "###########################################################\n",
    "# Q: do we need to shuffle the dataset? What is drop_last?\n",
    "###########################################################\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2, shuffle=..., drop_last=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxUz7SddZhDb"
   },
   "source": [
    "Next, we need to define our RNN model before training. For this, we will implement the RNN Cell discussed in lecture 8 slide 6. Note that the RNN Cell operates on a single timestep. So, the RNN Cell will take a single timestep token as input and produce output and hidden state for that particular timestep only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XW6an9XZhDb"
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TO-DO: Implement the __init__ and forward methods of the RNNCell class. \n",
    "# Refer to the equations in the lecture and implement the same here\n",
    "# The forward method should return the output and the hidden state\n",
    "#########################################################################\n",
    "\n",
    "class RNNCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        \n",
    "    def forward(self, input_emb, hidden_state):\n",
    "        ...\n",
    "        ...\n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klDoyEDFZhDc"
   },
   "source": [
    "Since we have a sequence of tokens as input, we will implement another model which uses this RNNCell and processes multi-timestep sequence inputs. The RNN class takes in a sequence of one-hot encodings of tokens as input and returns a sequence of output, one for each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HYKOii-ZhDc"
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TO-DO: Implement the forward method of the RNN class. \n",
    "# The RNN class takes in a sequence of one-hot encodings of tokens as input \n",
    "# and returns a sequence of output, one for each timestep.\n",
    "# We also return the hidden state of the final timestep\n",
    "# Q: Is it required to return the hidden state? If yes, why? If no, why?\n",
    "#########################################################################\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_length, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = RNNCell(vocab_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        \n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        \n",
    "        return outputs, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rG_JzB1SZhDd"
   },
   "source": [
    "Now that dataset and model definitions are done, we need to implement the training loop and we are good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vel-IZ5_ZhDe"
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# TO-DO: Implement the missing part of the training function. \n",
    "# As a loss function we want to use cross-entropy\n",
    "# It can be called with F.cross_entropy().\n",
    "# Hint: Pass through the model -> Backpropagate gradients -> Take gradient step\n",
    "#########################################################################\n",
    "\n",
    "def train(model, dataloader, optimizer, epoch, log_interval, device, hidden_size):\n",
    "    model.train()\n",
    "    \n",
    "    # initialize hidden state to 0 at the beginning of the epoch \n",
    "    # and keep propagating it to the next batch\n",
    "    hidden_state = torch.zeros(batch_size, hidden_size).to(device)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # loss can be computed in 2 ways:\n",
    "        # mean across seq_length and batch_size or sum across seq_length and mean across batch_size\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbBlJPMfZhDe"
   },
   "source": [
    "Next, we instantiate our model and optimizer and then we can start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOkvczu7ZhDf"
   },
   "outputs": [],
   "source": [
    "# model and training parameters\n",
    "# feel free to experiment with different parameters and optimizers\n",
    "current_hidden_size = 256\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda'\n",
    "\n",
    "rnn = RNN(seq_length, vocab_size, current_hidden_size).to(device)\n",
    "\n",
    "optimizer = optim.RMSprop(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZKOSONEZhDf"
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "epochs = 1\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(rnn, dataloader, optimizer, epoch, log_interval=1000, device=device, hidden_size=current_hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuwNe4t0ZhDf"
   },
   "source": [
    "Along with training the model, it's also good to check what kind of text our model is generating. We have implemented a function which samples text from the model give an initial token and a hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcSL1GOdZhDf"
   },
   "outputs": [],
   "source": [
    "# sample text of length seq_len, this seq_len does not need to be the same\n",
    "# as seq_length that we used earlier, we can basically sample text of any arbitrary length.\n",
    "\n",
    "softmax_temp = 0.3\n",
    "\n",
    "def sample(hidden_state, token, seq_len):\n",
    "    token_emb = torch.zeros(1, vocab_size).to(device) # use batch_size=1 at inference\n",
    "    token_emb[0,token] = 1\n",
    "    char_indices = [token] # first token\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for timestep in range(seq_len):\n",
    "            output, hidden_state = rnn.rnn_cell(token_emb, hidden_state)\n",
    "            output = torch.softmax(output / softmax_temp, dim=-1) # convert to probabilities\n",
    "            token = torch.multinomial(output.squeeze(), num_samples=1).item()  # sample token from output distribution\n",
    "            char_indices.append(token)\n",
    "\n",
    "            token_emb = torch.zeros(1, vocab_size).to(device)\n",
    "            token_emb[0, token] = 1\n",
    "    \n",
    "    return char_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdLmCM3DZhDg"
   },
   "source": [
    "Now, let's sample sample text from the model after every epoch to see if our model is learning to generate some text or not. In the code below, we are sampling a 20 char text from the model, starting with a random token and 0 memory. Try to generate some text by using the the hidden_state returned by RNN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2EN_b5RZhDg"
   },
   "outputs": [],
   "source": [
    "# sample a 20-char text from the model, starting with a random token and 0 memory\n",
    "for i in range(10):\n",
    "  token = np.random.randint(0, vocab_size)\n",
    "  hidden_state = torch.zeros(1, current_hidden_size).to(device)\n",
    "  char_indices = sample(hidden_state, token, 50) # sample a 50 char text from the model\n",
    "  txt = ''.join(dataset.ix_to_char[ix] for ix in char_indices)\n",
    "  print (txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlVvphecZhDg"
   },
   "source": [
    "If everything went well, then our model should be able to generate some legible text after some epochs. However, it'd probably be quite slow. The model should be able to learn spellings and certain words, use of spaces and how to begin sentences. It most likely won't be able to generate long sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJZr3P0wZhDg"
   },
   "source": [
    "Bonus: Try implementing a word level language model with the same model on the same dataset. The only difference is that now each word is a token rather than each character. So, the tokenization in the dataset needs to be changed and everything else remains same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dbtMCJjZhDg"
   },
   "source": [
    "Bonus: Try using a 3-layer RNN with 512 dimensional hidden state and see if the model is able to generate better text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaO16GYF49Pc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
